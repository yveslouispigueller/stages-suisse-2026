#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scraper pour sites d'emploi SUISSES
Sites : Jobs.ch, Jobup.ch, Travail.swiss, eFinancialCareers.ch
Version adaptÃ©e aux dÃ©butants
"""

import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time

def charger_donnees():
    """Charge le fichier JSON"""
    try:
        with open('stages_data.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {"derniere_maj": "", "stages": []}

def sauvegarder_donnees(data):
    """Sauvegarde dans le fichier JSON"""
    with open('stages_data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def get_headers():
    """Headers pour les requÃªtes HTTP"""
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9,fr;q=0.8',
    }

def scraper_jobs_ch():
    """
    Scraper pour Jobs.ch
    Un des plus grands sites d'emploi en Suisse
    """
    print("\nğŸ” [1/4] Scraping Jobs.ch...")
    print("-" * 70)
    stages = []
    
    try:
        recherches = [
            'finance+internship+zurich',
            'stage+finance+geneva',
            'trainee+finance+basel'
        ]
        
        for terme in recherches:
            print(f"  ğŸ” Recherche : {terme.replace('+', ' ')}")
            
            url = f"https://www.jobs.ch/en/vacancies/?term={terme}"
            
            try:
                time.sleep(2)
                response = requests.get(url, headers=get_headers(), timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Chercher tous les liens d'offres
                    links = soup.find_all('a', href=True)
                    job_links = [link for link in links if '/job/' in link.get('href', '')]
                    
                    for link in job_links[:5]:
                        try:
                            title = link.get_text(strip=True)
                            parent = link.find_parent()
                            
                            # Chercher l'entreprise
                            company = "Entreprise non spÃ©cifiÃ©e"
                            if parent:
                                company_elem = parent.find('span', class_=lambda x: x and 'company' in str(x).lower())
                                if company_elem:
                                    company = company_elem.get_text(strip=True)
                            
                            if len(title) > 10:
                                stage = {
                                    "company": company,
                                    "title": title,
                                    "domain": "Finance",
                                    "location": "Switzerland",
                                    "duration": "6 mois",
                                    "startDate": "Variable",
                                    "link": f"https://www.jobs.ch{link['href']}" if not link['href'].startswith('http') else link['href']
                                }
                                stages.append(stage)
                        except:
                            continue
                    
                    print(f"  âœ“ {len([s for s in stages])} offres trouvÃ©es pour cette recherche")
                else:
                    print(f"  âš ï¸  Statut HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"  âŒ Erreur : {str(e)[:60]}")
            
    except Exception as e:
        print(f"  âŒ Erreur gÃ©nÃ©rale : {e}")
    
    print(f"  ğŸ“Š Total Jobs.ch : {len(stages)} offres")
    return stages

def scraper_jobup_ch():
    """
    Scraper pour Jobup.ch
    Plateforme suisse romande et alÃ©manique
    """
    print("\nğŸ” [2/4] Scraping Jobup.ch...")
    print("-" * 70)
    stages = []
    
    try:
        recherches = [
            'finance+internship',
            'stage+finance',
            'financial+analyst+trainee'
        ]
        
        for terme in recherches:
            print(f"  ğŸ” Recherche : {terme.replace('+', ' ')}")
            
            url = f"https://www.jobup.ch/en/jobs/?term={terme}"
            
            try:
                time.sleep(2)
                response = requests.get(url, headers=get_headers(), timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Chercher les articles ou divs de jobs
                    job_elements = soup.find_all(['article', 'div'], class_=lambda x: x and 'job' in str(x).lower())
                    
                    for job in job_elements[:5]:
                        try:
                            # Chercher titre
                            title_elem = job.find(['h2', 'h3', 'a'])
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text(strip=True)
                            
                            # Chercher entreprise
                            company_elem = job.find(['span', 'div', 'p'], class_=lambda x: x and ('company' in str(x).lower() or 'employer' in str(x).lower()))
                            company = company_elem.get_text(strip=True) if company_elem else "Entreprise non spÃ©cifiÃ©e"
                            
                            # Chercher lien
                            link_elem = job.find('a', href=True)
                            link = link_elem['href'] if link_elem else url
                            if not link.startswith('http'):
                                link = f"https://www.jobup.ch{link}"
                            
                            # Chercher localisation
                            location_elem = job.find(['span', 'div'], class_=lambda x: x and 'location' in str(x).lower())
                            location = location_elem.get_text(strip=True) if location_elem else "Switzerland"
                            
                            if len(title) > 10:
                                stage = {
                                    "company": company,
                                    "title": title,
                                    "domain": "Finance",
                                    "location": location,
                                    "duration": "6 mois",
                                    "startDate": "Variable",
                                    "link": link
                                }
                                stages.append(stage)
                        except:
                            continue
                    
                    print(f"  âœ“ {len([s for s in stages])} offres trouvÃ©es pour cette recherche")
                else:
                    print(f"  âš ï¸  Statut HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"  âŒ Erreur : {str(e)[:60]}")
            
    except Exception as e:
        print(f"  âŒ Erreur gÃ©nÃ©rale : {e}")
    
    print(f"  ğŸ“Š Total Jobup.ch : {len(stages)} offres")
    return stages

def scraper_travail_swiss():
    """
    Scraper pour Travail.swiss
    Portail officiel du SECO (SecrÃ©tariat d'Ã‰tat Ã  l'Ã©conomie)
    """
    print("\nğŸ” [3/4] Scraping Travail.swiss...")
    print("-" * 70)
    stages = []
    
    try:
        # Recherches en franÃ§ais et anglais
        recherches = [
            'finance+internship',
            'stage+finance',
            'stagiaire+finance'
        ]
        
        for terme in recherches:
            print(f"  ğŸ” Recherche : {terme.replace('+', ' ')}")
            
            # URL du portail officiel
            url = f"https://www.travail.swiss/job-search/?keywords={terme}"
            
            try:
                time.sleep(2)
                response = requests.get(url, headers=get_headers(), timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Structure du site officiel
                    job_listings = soup.find_all(['article', 'li', 'div'], class_=lambda x: x and ('job' in str(x).lower() or 'listing' in str(x).lower()))
                    
                    for job in job_listings[:5]:
                        try:
                            # Titre
                            title_elem = job.find(['h2', 'h3', 'h4', 'a', 'strong'])
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text(strip=True)
                            
                            # Entreprise
                            company_elem = job.find(['span', 'div', 'p'], text=lambda t: t and ('SA' in str(t) or 'AG' in str(t) or 'GmbH' in str(t) or 'Ltd' in str(t)))
                            if not company_elem:
                                company_elem = job.find(['span', 'div'], class_=lambda x: x and 'company' in str(x).lower())
                            company = company_elem.get_text(strip=True) if company_elem else "Entreprise non spÃ©cifiÃ©e"
                            
                            # Lien
                            link_elem = job.find('a', href=True)
                            link = link_elem['href'] if link_elem else url
                            if not link.startswith('http'):
                                link = f"https://www.travail.swiss{link}"
                            
                            # Location
                            location = "Switzerland"
                            for text in job.stripped_strings:
                                if any(city in text for city in ['Zurich', 'Geneva', 'GenÃ¨ve', 'Lausanne', 'Basel', 'Bern', 'Berne']):
                                    location = text.strip()
                                    break
                            
                            if len(title) > 10:
                                stage = {
                                    "company": company,
                                    "title": title,
                                    "domain": "Finance",
                                    "location": location,
                                    "duration": "6 mois",
                                    "startDate": "Variable",
                                    "link": link
                                }
                                stages.append(stage)
                        except:
                            continue
                    
                    print(f"  âœ“ {len([s for s in stages])} offres trouvÃ©es pour cette recherche")
                else:
                    print(f"  âš ï¸  Statut HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"  âŒ Erreur : {str(e)[:60]}")
            
    except Exception as e:
        print(f"  âŒ Erreur gÃ©nÃ©rale : {e}")
    
    print(f"  ğŸ“Š Total Travail.swiss : {len(stages)} offres")
    return stages

def scraper_efinancialcareers():
    """
    Scraper pour eFinancialCareers.ch
    Site spÃ©cialisÃ© dans les emplois finance
    """
    print("\nğŸ” [4/4] Scraping eFinancialCareers.ch...")
    print("-" * 70)
    stages = []
    
    try:
        recherches = [
            'internship',
            'trainee',
            'graduate'
        ]
        
        for terme in recherches:
            print(f"  ğŸ” Recherche : {terme}")
            
            # URL spÃ©cialisÃ©e finance
            url = f"https://www.efinancialcareers.ch/jobs/search?keywords={terme}&location=Switzerland"
            
            try:
                time.sleep(2)
                response = requests.get(url, headers=get_headers(), timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Structure eFinancialCareers
                    job_cards = soup.find_all(['article', 'div', 'li'], class_=lambda x: x and ('job' in str(x).lower() or 'result' in str(x).lower()))
                    
                    for card in job_cards[:5]:
                        try:
                            # Titre
                            title_elem = card.find(['h2', 'h3', 'a'], class_=lambda x: x and 'title' in str(x).lower())
                            if not title_elem:
                                title_elem = card.find(['h2', 'h3', 'a'])
                            
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text(strip=True)
                            
                            # Entreprise
                            company_elem = card.find(['span', 'div', 'p'], class_=lambda x: x and ('company' in str(x).lower() or 'employer' in str(x).lower()))
                            if not company_elem:
                                company_elem = card.find(['span', 'div'], text=lambda t: t and any(word in str(t) for word in ['Bank', 'Group', 'AG', 'SA']))
                            company = company_elem.get_text(strip=True) if company_elem else "Entreprise non spÃ©cifiÃ©e"
                            
                            # Lien
                            link_elem = card.find('a', href=True)
                            link = link_elem['href'] if link_elem else url
                            if not link.startswith('http'):
                                link = f"https://www.efinancialcareers.ch{link}"
                            
                            # Location
                            location_elem = card.find(['span', 'div'], class_=lambda x: x and 'location' in str(x).lower())
                            location = location_elem.get_text(strip=True) if location_elem else "Switzerland"
                            
                            # Filtrer pour garder seulement les stages/internships
                            if len(title) > 10 and any(word in title.lower() for word in ['intern', 'stage', 'trainee', 'graduate']):
                                stage = {
                                    "company": company,
                                    "title": title,
                                    "domain": "Finance",
                                    "location": location,
                                    "duration": "6 mois",
                                    "startDate": "Variable",
                                    "link": link
                                }
                                stages.append(stage)
                        except:
                            continue
                    
                    print(f"  âœ“ {len([s for s in stages])} offres trouvÃ©es pour cette recherche")
                else:
                    print(f"  âš ï¸  Statut HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"  âŒ Erreur : {str(e)[:60]}")
            
    except Exception as e:
        print(f"  âŒ Erreur gÃ©nÃ©rale : {e}")
    
    print(f"  ğŸ“Š Total eFinancialCareers : {len(stages)} offres")
    return stages

def nettoyer_doublons(stages):
    """Supprime les doublons basÃ©s sur entreprise + titre"""
    vus = set()
    uniques = []
    
    for stage in stages:
        # ClÃ© unique
        cle = (
            stage['company'].lower().strip(),
            stage['title'].lower().strip()
        )
        
        if cle not in vus and len(stage['title']) > 10:
            vus.add(cle)
            uniques.append(stage)
    
    return uniques

def fusionner_avec_existants(nouveaux, anciens):
    """Fusionne les nouvelles offres avec les anciennes"""
    nouveaux_dict = {}
    
    for stage in nouveaux:
        cle = (stage['company'].lower().strip(), stage['title'].lower().strip())
        nouveaux_dict[cle] = stage
    
    for ancien in anciens:
        cle = (ancien['company'].lower().strip(), ancien['title'].lower().strip())
        if cle not in nouveaux_dict:
            nouveaux_dict[cle] = ancien
    
    return list(nouveaux_dict.values())

def main():
    """Fonction principale"""
    print("="*70)
    print("ğŸ‡¨ğŸ‡­ SCRAPER SITES D'EMPLOI SUISSES - VERSION COMPLÃˆTE")
    print("="*70)
    print("\nğŸ“ Sites ciblÃ©s :")
    print("   1. Jobs.ch")
    print("   2. Jobup.ch")
    print("   3. Travail.swiss (portail officiel SECO)")
    print("   4. eFinancialCareers.ch")
    
    print("\nâš ï¸  IMPORTANT :")
    print("   â€¢ Le scraping peut Ã©chouer si les sites bloquent les robots")
    print("   â€¢ Les structures HTML changent rÃ©guliÃ¨rement")
    print("   â€¢ RÃ©sultats variables selon les sites")
    print("   â€¢ Pour de meilleurs rÃ©sultats : combinez avec l'ajout manuel\n")
    
    # Charger donnÃ©es
    data = charger_donnees()
    anciens_stages = data.get('stages', [])
    
    print(f"ğŸ“Š Base actuelle : {len(anciens_stages)} offres\n")
    
    input("Appuyez sur ENTRÃ‰E pour dÃ©marrer le scraping... ")
    
    print("\n" + "="*70)
    print("ğŸš€ DÃ‰BUT DU SCRAPING")
    print("="*70)
    
    # Collecter toutes les offres
    tous_nouveaux = []
    
    # 1. Jobs.ch
    tous_nouveaux.extend(scraper_jobs_ch())
    
    # 2. Jobup.ch
    tous_nouveaux.extend(scraper_jobup_ch())
    
    # 3. Travail.swiss
    tous_nouveaux.extend(scraper_travail_swiss())
    
    # 4. eFinancialCareers
    tous_nouveaux.extend(scraper_efinancialcareers())
    
    print("\n" + "="*70)
    print("ğŸ§¹ NETTOYAGE DES DOUBLONS")
    print("="*70)
    
    # Nettoyer doublons
    avant_nettoyage = len(tous_nouveaux)
    tous_nouveaux = nettoyer_doublons(tous_nouveaux)
    print(f"  Avant : {avant_nettoyage} offres")
    print(f"  AprÃ¨s : {len(tous_nouveaux)} offres")
    print(f"  Doublons supprimÃ©s : {avant_nettoyage - len(tous_nouveaux)}")
    
    # RÃ©sultats
    print("\n" + "="*70)
    print("ğŸ“Š RÃ‰SULTATS DU SCRAPING")
    print("="*70)
    
    if tous_nouveaux:
        print(f"\nâœ¨ {len(tous_nouveaux)} nouvelles offres trouvÃ©es !\n")
        
        # AperÃ§u
        print("ğŸ“‹ AperÃ§u des offres :")
        for i, stage in enumerate(tous_nouveaux[:10], 1):
            print(f"\n   {i}. {stage['title'][:65]}")
            print(f"      ğŸ¢ {stage['company']}")
            print(f"      ğŸ“ {stage['location']}")
        
        if len(tous_nouveaux) > 10:
            print(f"\n   ... et {len(tous_nouveaux) - 10} autres offres")
        
        # Fusionner
        tous_stages = fusionner_avec_existants(tous_nouveaux, anciens_stages)
        
        print(f"\nğŸ“ˆ Statistiques :")
        print(f"   â€¢ Nouvelles offres : {len(tous_nouveaux)}")
        print(f"   â€¢ Anciennes offres : {len(anciens_stages)}")
        print(f"   â€¢ Total final : {len(tous_stages)}")
        print(f"   â€¢ Gain : +{len(tous_stages) - len(anciens_stages)} offres")
        
        # Sauvegarder
        now = datetime.now()
        data['derniere_maj'] = now.strftime("%d %B %Y - %H:%M")
        data['stages'] = tous_stages
        
        sauvegarder_donnees(data)
        
        print("\nâœ… Fichier stages_data.json mis Ã  jour !")
        
        # Stats par domaine
        print("\n" + "="*70)
        print("ğŸ“Š STATISTIQUES PAR DOMAINE")
        print("="*70)
        
        domaines = {}
        for stage in tous_stages:
            domain = stage.get('domain', 'Non spÃ©cifiÃ©')
            domaines[domain] = domaines.get(domain, 0) + 1
        
        for domain, count in sorted(domaines.items(), key=lambda x: x[1], reverse=True):
            print(f"   â€¢ {domain:25} : {count} offres")
        
    else:
        print("\nâš ï¸  Aucune nouvelle offre trouvÃ©e")
        print("\nğŸ’¡ Raisons possibles :")
        print("   â€¢ Les sites bloquent les robots")
        print("   â€¢ Aucune offre disponible actuellement")
        print("   â€¢ Structure HTML des sites changÃ©e")
        print("\nğŸ“ Solution : Utilisez l'outil d'ajout manuel")
    
    print("\n" + "="*70)
    print("ğŸ‰ SCRAPING TERMINÃ‰ !")
    print("="*70)
    
    print("\nğŸ“Œ Prochaines Ã©tapes :")
    print("   1. VÃ©rifiez stages_data.json")
    print("   2. Uploadez sur GitHub")
    print("   3. Votre site sera mis Ã  jour automatiquement")
    print("\nğŸ’¡ Conseil : Lancez ce script 1-2 fois par semaine maximum\n")

if __name__ == "__main__":
    main()
